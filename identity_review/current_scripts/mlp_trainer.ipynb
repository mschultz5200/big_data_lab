{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "from random import randint\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/Users/matthewschultz/Big_Data_Lab/identity_review/csv_output/test_data.csv')\n",
    "\n",
    "def create_equal_representation(df):\n",
    "    malicious = df.loc[df['output'] == 1]\n",
    "\n",
    "    non_mal = df.loc[df['output'] == 0]\n",
    "    non_mal_sample = non_mal.sample(len(malicious))\n",
    "\n",
    "    return pd.concat([malicious, non_mal_sample], ignore_index= True)\n",
    "\n",
    "equalized_df = create_equal_representation(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in the pickle functions here \n",
    "def normalize(df):\n",
    "    scalar = preprocessing.Normalizer(norm=\"l2\").fit(df[['num_of_mal_trans', 'ratio']])\n",
    "    pickle.dump(scalar, open(\"/Users/matthewschultz/Big_Data_Lab/identity_review/mlp_normalizer/scalar_mlp.save\", \"wb\"))\n",
    "    scaled = scalar.transform(df[['num_of_mal_trans', 'ratio']])\n",
    "    temp = df[['value_out', 'value_out', 'avg_trans', 'num_of_zero', 'output']].to_numpy()\n",
    "\n",
    "    combined = np.concatenate((scaled, temp), axis = 1)\n",
    "\n",
    "    normalized = pd.DataFrame(combined)\n",
    "    normalized = normalized.rename(columns = {\n",
    "        0: 'num_of_mal_trans',\n",
    "        1: 'ratio',\n",
    "        2: 'value_out',\n",
    "        3: 'value_in',\n",
    "        4: 'avg_trans',\n",
    "        5: 'num_of_zero',\n",
    "        6: 'output'\n",
    "    })\n",
    "    return normalized\n",
    "\n",
    "normalzed_df = normalize(equalized_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am looking to run a test that compares the normalized data to the non-normaized data to see which data creates a better model. Then we go into pruning and other validation test. This is esstentially which model will have better results when running the test first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_non_normalized = equalized_df[['num_of_mal_trans', 'ratio', 'value_out', 'value_in', 'avg_trans', 'num_of_zero']].to_numpy()\n",
    "y_non_normalized = equalized_df['output'].to_numpy()\n",
    "\n",
    "X_normalized = normalzed_df[['num_of_mal_trans', 'ratio', 'value_out', 'value_in', 'avg_trans', 'num_of_zero']].to_numpy()\n",
    "y_normalized = normalzed_df['output'].to_numpy()\n",
    "\n",
    "\n",
    "def ensure_equal(x_df, y_df):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_df, y_df, train_size=0.65, random_state = randint(1, 300))\n",
    "    if len(np.where(y_train == 1)[0]) / len(y_train) < 0.5:\n",
    "        return ensure_equal(x_df, y_df)\n",
    "    else:\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "X_non_train, X_non_test, y_non_train, y_non_test = ensure_equal(X_non_normalized, y_non_normalized)\n",
    "X_norm_train, X_norm_test, y_norm_train, y_norm_test = ensure_equal(X_normalized, y_normalized)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for the non-normalized data. Set activation to logitic because this is a binary classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5        0.71428571 0.77777778 0.55555556]\n",
      "[[ 1 19]\n",
      " [ 1 18]]\n"
     ]
    }
   ],
   "source": [
    "clf_non = MLPClassifier(hidden_layer_sizes = (10,), \n",
    "                        activation = 'logistic',\n",
    "                        solver = 'sgd',\n",
    "                        learning_rate = 'adaptive',\n",
    "                        max_iter = 2000,\n",
    "                        random_state=1)\n",
    "cros_val_non = cross_val_score(clf_non, X_non_normalized, y_non_normalized, cv=4)\n",
    "\n",
    "fitted_non = clf_non.fit(X_non_train, y_non_train)\n",
    "\n",
    "predict_non = fitted_non.predict(X_non_test).tolist()\n",
    "\n",
    "conf_matrix_non = confusion_matrix(y_non_test, predict_non)\n",
    "\n",
    "print(cros_val_non)\n",
    "print(conf_matrix_non)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be for the normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.81818182 0.81818182 0.90909091 1.         0.81818182 0.90909091\n",
      " 1.         0.81818182 0.81818182 0.81818182]\n",
      "[[17  4]\n",
      " [ 1 17]]\n"
     ]
    }
   ],
   "source": [
    "clf_norm = MLPClassifier(hidden_layer_sizes = (13,), \n",
    "                        activation = 'logistic',\n",
    "                        solver = 'sgd',\n",
    "                        learning_rate = 'adaptive',\n",
    "                        max_iter = 2000,\n",
    "                        random_state=1)\n",
    "cros_val_norm = cross_val_score(clf_norm, X_normalized, y_normalized, cv=10)\n",
    "\n",
    "fitted_norm = clf_norm.fit(X_norm_train, y_norm_train)\n",
    "\n",
    "predict_norm = fitted_norm.predict(X_norm_test).tolist()\n",
    "\n",
    "conf_matrix_norm = confusion_matrix(y_norm_test, predict_norm)\n",
    "\n",
    "print(cros_val_norm)\n",
    "print(conf_matrix_norm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the test, I have concluded that the normalized version of the model is better optimized for the data set. It is important to note that the unnormalized model was performing better until I added the value in and out features. However, I think that it is important to include these features because it helps visualize the disparity between non-malicious and malicious accounts. For example, most non-malicious accounts will have a very similar input and output amounts making it easier to identify.\n",
    "\n",
    "I want to find the optimal number of nodes in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "   num_hidden  average_cross  average_pos  average_neg\n",
      "0           5       0.809091     9.965517     1.172414\n",
      "0           6       0.809091     7.655172     0.310345\n",
      "0           7       0.872727     3.034483     3.068966\n",
      "0           8       0.790909     6.103448     1.482759\n",
      "0           9       0.818182     7.896552     0.310345\n",
      "0          10       0.781818    11.206897     0.275862\n",
      "0          11       0.800000     8.517241     0.482759\n",
      "0          12       0.763636     7.310345     0.448276\n",
      "0          13       0.872727     5.068966     0.862069\n",
      "0          14       0.809091     8.275862     0.241379\n",
      "0          15       0.863636     5.862069     0.344828\n"
     ]
    }
   ],
   "source": [
    "def find_optimal_model(X, y):\n",
    "    number_of_nodes = range(5,16)\n",
    "    list_of_df = []\n",
    "    for i in number_of_nodes:\n",
    "        print(i)\n",
    "        clf_norm = MLPClassifier(hidden_layer_sizes = (i,), \n",
    "            activation = 'logistic',\n",
    "            solver = 'sgd',\n",
    "            learning_rate = 'adaptive',\n",
    "            max_iter = 2500,\n",
    "            random_state=1)\n",
    "        average_cross = np.mean(cross_val_score(clf_norm, X, y, cv=10)) \n",
    "        list_of_false_positives = []\n",
    "        list_of_false_negatives = [] \n",
    "        for k in range(0, 29):\n",
    "            X_norm_train, X_norm_test, y_norm_train, y_norm_test = ensure_equal(X, y)\n",
    "            fitted_norm = clf_norm.fit(X_norm_train, y_norm_train)\n",
    "            predict_norm = fitted_norm.predict(X_norm_test).tolist()\n",
    "            conf_matrix_norm = confusion_matrix(y_norm_test, predict_norm)\n",
    "            list_of_false_positives.append(conf_matrix_norm[0, 1])\n",
    "            list_of_false_negatives.append(conf_matrix_norm[1, 0])\n",
    "        list_of_df.append(pd.DataFrame({'num_hidden': [i],\n",
    "                                        'average_cross': [average_cross],\n",
    "                                        'average_pos': [sum(list_of_false_positives)/len(list_of_false_positives)],\n",
    "                                        'average_neg': [sum(list_of_false_negatives)/len(list_of_false_negatives)]}))\n",
    "    final = pd.concat(list_of_df)\n",
    "    return final\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "temp = find_optimal_model(X_normalized, y_normalized)\n",
    "print(temp)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trained_model(model, path):\n",
    "    list_files = os.listdir(path)\n",
    "    files = [file for file in list_files if file != '.DS_Store']\n",
    "    if len(files) == 0:\n",
    "        file_name = path + \"/mlp_1.sav\"\n",
    "        pickle.dump(model, open(file_name, 'wb'))\n",
    "    else:\n",
    "        file_number = int(files[-1].split(\"_\")[2].split(\".\")[0]) + 1\n",
    "        file_name = path + \"/mlp_{}.sav\".format(file_number)\n",
    "        pickle.dump(model, open(file_name, 'wb'))\n",
    "        \n",
    "\n",
    "path_name = '/Users/matthewschultz/Big_Data_Lab/identity_review/mlp_iteration'\n",
    "save_trained_model(clf_norm, path_name)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
